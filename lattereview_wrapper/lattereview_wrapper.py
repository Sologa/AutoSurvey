#!/usr/bin/env python3
"""
LatteReview Wrapper for AutoSurvey
å°è£…LatteReviewçš„è¯„å®¡æµç¨‹ï¼Œæä¾›ç®€å•çš„å‡½æ•°æ¥å£
"""

import asyncio
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import pandas as pd

# æ·»åŠ LatteReviewè·¯å¾„
latte_review_path = Path(__file__).parent.parent / "LatteReview"
sys.path.append(str(latte_review_path))

from lattereview.providers import OpenAIProvider, GoogleProvider, OllamaProvider, LiteLLMProvider
from lattereview.agents import TitleAbstractReviewer
from lattereview.workflows import ReviewWorkflow


def create_provider(model_name: str):
    """åˆ›å»ºæ¨¡å‹æä¾›è€…"""
    if model_name.startswith("gpt-"):
        return OpenAIProvider(model=model_name)
    elif model_name.startswith("claude-"):
        return GoogleProvider(model=model_name)
    elif model_name.startswith("gemini-"):
        return GoogleProvider(model=model_name)
    elif model_name.startswith("o4-") or model_name.startswith("o3-"):
        return OpenAIProvider(model=model_name)
    elif model_name.startswith("llama-") or model_name.startswith("mistral-"):
        return OllamaProvider(model=model_name)
    else:
        # é»˜è®¤ä½¿ç”¨LiteLLM
        return LiteLLMProvider(model=model_name)


def create_reviewers(
    reviewer_models: List[str],
    topic: str,
    inclusion_criteria: Optional[str] = None,
    exclusion_criteria: Optional[str] = None
) -> List[TitleAbstractReviewer]:
    """åˆ›å»ºè¯„å®¡è€…"""
    reviewers = []
    
    # é»˜è®¤è¯„å®¡æ ‡å‡†
    if inclusion_criteria is None:
        inclusion_criteria = f"""
        Research must be related to the following topic:
        {topic}
        
        The paper should demonstrate:
        1. Clear research contribution
        2. Well-defined methodology
        3. Relevant to the specified topic area
        4. Academic quality and rigor
        """
    
    if exclusion_criteria is None:
        exclusion_criteria = """
        Exclude the following research:
        1. Non-English papers
        2. Non-peer-reviewed papers
        3. Research unrelated to the specified topic
        4. Pure theoretical research without practical application
        5. Duplicate papers
        6. Non-academic content
        """
    
    # è¯„å®¡è€…é…ç½®
    reviewer_configs = [
        {
            "name": "Conservative_Reviewer",
            "backstory": "Conservative review expert who strictly follows inclusion criteria",
            "reasoning": "brief",
            "model": reviewer_models[0] if len(reviewer_models) > 0 else "gpt-4o-mini"
        },
        {
            "name": "Balanced_Reviewer", 
            "backstory": "Balanced review expert who considers multiple factors comprehensively",
            "reasoning": "cot",
            "model": reviewer_models[1] if len(reviewer_models) > 1 else "gpt-4.1-mini"
        },
        {
            "name": "Senior_Reviewer",
            "backstory": "Senior AI research expert with extensive experience in the field",
            "reasoning": "cot", 
            "model": reviewer_models[2] if len(reviewer_models) > 2 else "gpt-5-mini"
        }
    ]
    
    for i, config in enumerate(reviewer_configs[:len(reviewer_models)]):
        # ä¸ºæ¯ä¸ªè¯„å®¡è€…åˆ›å»ºä¸“é—¨çš„provider
        provider = create_provider(config["model"])
        
        # è®¾ç½®temperatureå‚æ•°
        model_args = {}
        if config["model"].startswith("gpt-5"):
            model_args["temperature"] = 1.0  # gpt-5å¼€å¤´çš„æ¨¡å‹é”å®štemperatureä¸º1.0
        else:
            model_args["temperature"] = 0.2  # å…¶ä»–éreasoning LLMè®¾ç½®ä¸º0.2
        
        reviewer = TitleAbstractReviewer(
            provider=provider,
            name=config["name"],
            backstory=config["backstory"],
            inclusion_criteria=inclusion_criteria,
            exclusion_criteria=exclusion_criteria,
            reasoning=config["reasoning"],
            model_args=model_args
        )
        
        reviewers.append(reviewer)
        print(f"Created reviewer {i+1}: {config['name']} (using model: {config['model']})")
    
    return reviewers


def create_workflow(reviewers: List[TitleAbstractReviewer]) -> ReviewWorkflow:
    """åˆ›å»ºè¯„å®¡å·¥ä½œæµ"""
    if len(reviewers) >= 3:
        # å®ç°æ­£ç¡®çš„ä¸¤é˜¶æ®µè¯„å®¡é€»è¾‘
        workflow_schema = [
            {
                "round": "A",
                "reviewers": reviewers[:2],  # å‰ä¸¤ä¸ªè¯„å®¡è€…ï¼ˆConservative + Balancedï¼‰
                "text_inputs": ["title", "abstract"],
                "filter": lambda x: True,
            },
            {
                "round": "B", 
                "reviewers": [reviewers[2]],  # ç¬¬ä¸‰ä¸ªè¯„å®¡è€…ï¼ˆSeniorï¼‰
                "text_inputs": ["title", "abstract"],
                "filter": lambda x: _has_disagreement(x),  # åªæœ‰å½“æœ‰åˆ†æ­§æ—¶æ‰è¯„å®¡
            }
        ]
    else:
        # å¦‚æœè¯„å®¡è€…å°‘äº3ä¸ªï¼Œä½¿ç”¨å•è½®è¯„å®¡
        workflow_schema = [
            {
                "round": "A",
                "reviewers": reviewers,  # æ‰€æœ‰è¯„å®¡è€…
                "text_inputs": ["title", "abstract"],
                "filter": lambda x: True,
            }
        ]
    
    return ReviewWorkflow(workflow_schema=workflow_schema, verbose=True)


def _has_disagreement(row: pd.Series) -> bool:
    """æ£€æŸ¥ä¸¤ä¸ªjuniorè¯„å®¡è€…æ˜¯å¦éœ€è¦å‡çº§åˆ°Round-B
    
    å‡çº§æ¡ä»¶ï¼ˆç¬¦åˆLatteReviewè¦æ±‚ï¼‰ï¼š
    1. ä¸¤ä½junioråˆ†æ•°ä¸ä¸€è‡´
    2. ä¸¤ä½junioréƒ½çµ¦3åˆ†ï¼ˆä¸ç¢ºå®šï¼‰
    """
    conservative_col = "round-A_Conservative_Reviewer_evaluation"
    balanced_col = "round-A_Balanced_Reviewer_evaluation"
    
    if conservative_col in row and balanced_col in row:
        try:
            conservative_score = _extract_score(row[conservative_col])
            balanced_score = _extract_score(row[balanced_col])
            
            if conservative_score is not None and balanced_score is not None:
                # æ¡ä»¶1: åˆ†æ•°ä¸ä¸€è‡´
                if abs(conservative_score - balanced_score) >= 1:
                    return True
                
                # æ¡ä»¶2: éƒ½çµ¦3åˆ†ï¼ˆä¸ç¢ºå®šï¼‰
                if conservative_score == 3 and balanced_score == 3:
                    return True
                
        except:
            pass
    
    return False


def _extract_score(value) -> Optional[float]:
    """ä»è¯„å®¡ç»“æœä¸­æå–è¯„åˆ†"""
    if pd.isna(value):
        return None
    
    try:
        if isinstance(value, str):
            parsed = json.loads(value)
            if 'evaluation' in parsed:
                return float(parsed['evaluation'])
            else:
                return float(parsed)
        else:
            return float(value)
    except:
        return None


async def run_lattereview_evaluation(
    topic: str,
    reviewer_models: List[str],
    papers: List[Dict[str, Any]],
    inclusion_criteria: Optional[str] = None,
    exclusion_criteria: Optional[str] = None,
    output_dir: Optional[str] = None,
    top_n: Optional[int] = None,
    threshold_mode: str = "Balanced"
) -> Dict[str, Any]:
    """
    è¿è¡ŒLatteReviewè¯„å®¡æµç¨‹
    
    Args:
        topic: ç ”ç©¶ä¸»é¢˜
        reviewer_models: ä¸‰ä¸ªè¯„å®¡è€…ä½¿ç”¨çš„æ¨¡å‹åç§°åˆ—è¡¨
        papers: è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ç¯‡è®ºæ–‡åº”åŒ…å«titleå’Œabstractå­—æ®µ
        inclusion_criteria: å¯é€‰çš„çº³å…¥æ ‡å‡†
        exclusion_criteria: å¯é€‰çš„æ’é™¤æ ‡å‡†
        output_dir: å¯é€‰çš„è¾“å‡ºç›®å½•
        top_n: å¯é€‰çš„ï¼Œè¾“å‡ºå‰Nç¯‡è®ºæ–‡ï¼ˆæŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼‰
        threshold_mode: å¯é€‰çš„ï¼Œå…¥é€‰é—¨æ§›æ¨¡å¼ ("Sensitive", "Specific", "Balanced")
    
    Returns:
        åŒ…å«è¯„å®¡ç»“æœçš„å­—å…¸
    """
    try:
        print(f"å¼€å§‹LatteReviewè¯„å®¡æµç¨‹")
        print(f"ä¸»é¢˜: {topic}")
        print(f"è¯„å®¡è€…æ¨¡å‹: {reviewer_models}")
        print(f"è®ºæ–‡æ•°é‡: {len(papers)}")
        if top_n:
            print(f"è¾“å‡ºå‰ {top_n} ç¯‡è®ºæ–‡")
        print(f"å…¥é€‰é—¨æ§›æ¨¡å¼: {threshold_mode}")
        
        # éªŒè¯è¾“å…¥
        if len(reviewer_models) < 1:
            raise ValueError("è‡³å°‘éœ€è¦1ä¸ªè¯„å®¡è€…æ¨¡å‹")
        
        if len(papers) == 0:
            raise ValueError("è®ºæ–‡åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        # æ£€æŸ¥è®ºæ–‡æ ¼å¼
        for i, paper in enumerate(papers):
            if 'title' not in paper:
                raise ValueError(f"ç¬¬{i+1}ç¯‡è®ºæ–‡ç¼ºå°‘titleå­—æ®µ")
            if 'abstract' not in paper:
                raise ValueError(f"ç¬¬{i+1}ç¯‡è®ºæ–‡ç¼ºå°‘abstractå­—æ®µ")
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(papers)
        
        # åˆ›å»ºè¯„å®¡è€…
        print("\nåˆ›å»ºè¯„å®¡è€…...")
        reviewers = create_reviewers(
            reviewer_models=reviewer_models,
            topic=topic,
            inclusion_criteria=inclusion_criteria,
            exclusion_criteria=exclusion_criteria
        )
        
        # åˆ›å»ºå·¥ä½œæµ
        print("åˆ›å»ºè¯„å®¡å·¥ä½œæµ...")
        workflow = create_workflow(reviewers)
        
        # è¿è¡Œè¯„å®¡
        print(f"\nå¼€å§‹è¯„å®¡ {len(df)} ç¯‡è®ºæ–‡...")
        start_time = asyncio.get_event_loop().time()
        
        results = await workflow(df)
        
        end_time = asyncio.get_event_loop().time()
        duration = end_time - start_time
        
        # è·å–æˆæœ¬ä¿¡æ¯
        total_cost = workflow.get_total_cost()
        
        print(f"è¯„å®¡å®Œæˆ! è€—æ—¶: {duration:.2f} ç§’")
        print(f"æ€»æˆæœ¬: ${total_cost:.4f}")
        
        # åˆ†æç»“æœ
        analysis = analyze_results(results, topic, reviewer_models, top_n, threshold_mode)
        
        # ä¿å­˜ç»“æœï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼‰
        if output_dir:
            save_results(results, analysis, output_dir, topic)
        
        return {
            "success": True,
            "topic": topic,
            "reviewer_models": reviewer_models,
            "total_papers": len(results),
            "duration_seconds": duration,
            "total_cost": total_cost,
            "results": results.to_dict('records'),
            "analysis": analysis
        }
        
    except Exception as e:
        print(f"è¯„å®¡è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "topic": topic,
            "reviewer_models": reviewer_models
        }


def analyze_results(results: pd.DataFrame, topic: str, reviewer_models: List[str], top_n: Optional[int] = None, threshold_mode: str = "Balanced") -> Dict[str, Any]:
    """åˆ†æè¯„å®¡ç»“æœ
    
    Args:
        results: è¯„å®¡ç»“æœDataFrame
        topic: ç ”ç©¶ä¸»é¢˜
        reviewer_models: è¯„å®¡è€…æ¨¡å‹åˆ—è¡¨
        top_n: è¾“å‡ºå‰Nç¯‡è®ºæ–‡
        threshold_mode: å…¥é€‰é—¨æ§›æ¨¡å¼ ("Sensitive", "Specific", "Balanced")
    """
    # å®šä¹‰å…¥é€‰é—¨æ§›ï¼ˆç¬¦åˆLatteReviewè¦æ±‚ï¼‰
    threshold_values = {
        "Sensitive": 1.5,    # å¬å›ä¼˜å…ˆï¼Œé—¨æ§›è¾ƒä½
        "Specific": 3.0,     # å¹³è¡¡æ¨¡å¼
        "Balanced": 4.5      # ç²¾ç¡®ä¼˜å…ˆï¼Œé—¨æ§›è¾ƒé«˜
    }
    
    if threshold_mode not in threshold_values:
        raise ValueError(f"æ— æ•ˆçš„threshold_mode: {threshold_mode}ã€‚æ”¯æŒçš„æ¨¡å¼: {list(threshold_values.keys())}")
    
    threshold = threshold_values[threshold_mode]
    
    analysis = {
        "topic": topic,
        "reviewer_models": reviewer_models,
        "total_papers": len(results),
        "review_rounds": [],
        "score_distribution": {},
        "high_score_papers": [],
        "top_papers": [],  # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºçš„å‰Nç¯‡è®ºæ–‡
        "inclusion_analysis": {  # ğŸ†• æ–°å¢ï¼šå…¥é€‰åˆ†æ
            "threshold_mode": threshold_mode,
            "threshold_value": threshold,
            "included_papers": [],
            "excluded_papers": [],
            "inclusion_rate": 0.0
        }
    }
    
    # åˆ†ææ¯è½®è¯„å®¡
    for col in results.columns:
        if col.startswith('round-') and col.endswith('_evaluation'):
            round_name = col.split('_')[0]
            reviewer_name = '_'.join(col.split('_')[1:-1])
            
            if round_name not in [r["round"] for r in analysis["review_rounds"]]:
                analysis["review_rounds"].append({
                    "round": round_name,
                    "reviewers": []
                })
            
            round_data = next(r for r in analysis["review_rounds"] if r["round"] == round_name)
            evaluations = results[col].dropna()
            
            # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
            eval_dist = evaluations.value_counts().to_dict()
            eval_dist = {str(k): int(v) for k, v in eval_dist.items()}
            
            round_data["reviewers"].append({
                "name": reviewer_name,
                "total_reviews": int(len(evaluations)),
                "evaluation_distribution": eval_dist,
                "mean_evaluation": float(evaluations.mean()) if not pd.isna(evaluations.mean()) else None,
                "std_evaluation": float(evaluations.std()) if not pd.isna(evaluations.std()) else None
            })
    
    # è®¡ç®—æ¯ç¯‡è®ºæ–‡çš„æœ€ç»ˆåˆ†æ•°å¹¶åˆ†æ
    papers_with_final_scores = []
    included_count = 0
    
    for _, row in results.iterrows():
        # è®¡ç®—æœ€ç»ˆåˆ†æ•° S_final
        final_score = _calculate_final_score(row)
        
        if final_score is not None:
            paper_info = {
                "id": row.get('id', ''),
                "title": row.get('title', ''),
                "final_score": final_score,
                "all_scores": _extract_all_scores(row),
                "score_details": _get_score_details(row),
                "reasons": _extract_reasons(row),  # ğŸ†• æ–°å¢ï¼šæå–ç†ç”±
                "is_included": final_score >= threshold  # ğŸ†• æ–°å¢ï¼šæ˜¯å¦å…¥é€‰
            }
            
            papers_with_final_scores.append(paper_info)
            
            # æ£€æŸ¥æ˜¯å¦å…¥é€‰
            if paper_info["is_included"]:
                included_count += 1
                analysis["inclusion_analysis"]["included_papers"].append(paper_info)
            else:
                analysis["inclusion_analysis"]["excluded_papers"].append(paper_info)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºé«˜åˆ†è®ºæ–‡ï¼ˆâ‰¥4åˆ†ï¼‰
            if final_score >= 4:
                analysis["high_score_papers"].append(paper_info)
    
    # è®¡ç®—å…¥é€‰ç‡
    if papers_with_final_scores:
        analysis["inclusion_analysis"]["inclusion_rate"] = included_count / len(papers_with_final_scores)
    
    # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
    papers_with_final_scores.sort(key=lambda x: x['final_score'], reverse=True)
    
    # è®¾ç½®top_nçš„é»˜è®¤å€¼
    if top_n is None:
        top_n = len(papers_with_final_scores)
    
    # è·å–å‰Nç¯‡è®ºæ–‡
    analysis["top_papers"] = papers_with_final_scores[:top_n]
    
    # æŒ‰æœ€é«˜åˆ†æ’åºé«˜åˆ†è®ºæ–‡
    analysis["high_score_papers"].sort(key=lambda x: x['final_score'], reverse=True)
    
    return analysis


def _calculate_final_score(row: pd.Series) -> Optional[float]:
    """æŒ‰ç…§paperç®—æ³•è®¡ç®—æœ€ç»ˆåˆ†æ•° S_final"""
    try:
        # 1. è‹¥æœ‰ round-B_Senior_Reviewer_evaluation â†’ ç”¨å®ƒå½“ S_final
        senior_b_col = "round-B_Senior_Reviewer_evaluation"
        if senior_b_col in row and pd.notna(row[senior_b_col]):
            score = _extract_score(row[senior_b_col])
            if score is not None:
                return score
        
        # 2. å¦åˆ™è‹¥æœ‰ round-A_Senior_Reviewer_evaluation â†’ ç”¨å®ƒ
        senior_a_col = "round-A_Senior_Reviewer_evaluation"
        if senior_a_col in row and pd.notna(row[senior_a_col]):
            score = _extract_score(row[senior_a_col])
            if score is not None:
                return score
        
        # 3. å¦åˆ™å–ä¸¤ä½ junior çš„å¹³å‡
        conservative_col = "round-A_Conservative_Reviewer_evaluation"
        balanced_col = "round-A_Balanced_Reviewer_evaluation"
        
        conservative_score = _extract_score(row.get(conservative_col))
        balanced_score = _extract_score(row.get(balanced_col))
        
        if conservative_score is not None and balanced_score is not None:
            return (conservative_score + balanced_score) / 2
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªjuniorçš„åˆ†æ•°ï¼Œä½¿ç”¨å®ƒ
        if conservative_score is not None:
            return conservative_score
        if balanced_score is not None:
            return balanced_score
        
        return None
        
    except Exception as e:
        print(f"è®¡ç®—æœ€ç»ˆåˆ†æ•°æ—¶å‡ºé”™: {e}")
        return None


def _extract_all_scores(row: pd.Series) -> List[float]:
    """æå–æ‰€æœ‰è¯„å®¡åˆ†æ•°"""
    scores = []
    for col in row.index:
        if col.endswith('_evaluation'):
            try:
                value = row[col]
                if pd.notna(value):
                    score = _extract_score(value)
                    if score is not None:
                        scores.append(score)
            except:
                continue
    return scores


def _extract_reasons(row: pd.Series) -> Dict[str, str]:
    """æå–è¯„å®¡ç†ç”±ï¼ˆç¬¦åˆLatteReviewè¦æ±‚ï¼šæ¯ä¸ªåˆ†æ•°éƒ½è¦é™„ç†ç”±ï¼‰"""
    reasons = {}
    
    # æå–å„è½®è¯„å®¡çš„ç†ç”±
    for col in row.index:
        if col.endswith('_output') or col.endswith('_reasoning'):
            try:
                value = row[col]
                if pd.notna(value):
                    if isinstance(value, str):
                        try:
                            # å°è¯•è§£æJSONæ ¼å¼çš„è¾“å‡º
                            parsed = json.loads(value)
                            if 'reasoning' in parsed:
                                reasons[col] = parsed['reasoning']
                            elif 'reason' in parsed:
                                reasons[col] = parsed['reason']
                            else:
                                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°reasoningå­—æ®µï¼Œä½¿ç”¨æ•´ä¸ªè¾“å‡º
                                reasons[col] = str(parsed)
                        except:
                            # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²
                            reasons[col] = value
                    else:
                        reasons[col] = str(value)
            except:
                continue
    
    return reasons


def _get_score_details(row: pd.Series) -> Dict[str, Any]:
    """è·å–è¯¦ç»†çš„è¯„åˆ†ä¿¡æ¯"""
    details = {}
    
    # æå–å„è½®è¯„å®¡çš„åˆ†æ•°
    for col in row.index:
        if col.endswith('_evaluation'):
            try:
                value = row[col]
                if pd.notna(value):
                    score = _extract_score(value)
                    if score is not None:
                        details[col] = score
            except:
                continue
    
    return details


def save_results(results: pd.DataFrame, analysis: Dict[str, Any], output_dir: str, topic: str):
    """ä¿å­˜è¯„å®¡ç»“æœ"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜åŸå§‹ç»“æœ
    results_file = output_path / f"lattereview_results_{topic.replace(' ', '_')}.json"
    results.to_json(results_file, orient='records', indent=2, force_ascii=False)
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_file = output_path / f"lattereview_analysis_{topic.replace(' ', '_')}.json"
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, indent=2, ensure_ascii=False)
    
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"  - è¯„å®¡ç»“æœ: {results_file.name}")
    print(f"  - åˆ†æç»“æœ: {analysis_file.name}")


# åŒæ­¥ç‰ˆæœ¬çš„åŒ…è£…å‡½æ•°
def run_lattereview_evaluation_sync(
    topic: str,
    reviewer_models: List[str],
    papers: List[Dict[str, Any]],
    inclusion_criteria: Optional[str] = None,
    exclusion_criteria: Optional[str] = None,
    output_dir: Optional[str] = None,
    top_n: Optional[int] = None,
    threshold_mode: str = "Balanced"
) -> Dict[str, Any]:
    """
    åŒæ­¥ç‰ˆæœ¬çš„LatteReviewè¯„å®¡å‡½æ•°
    
    Args:
        topic: ç ”ç©¶ä¸»é¢˜
        reviewer_models: ä¸‰ä¸ªè¯„å®¡è€…ä½¿ç”¨çš„æ¨¡å‹åç§°åˆ—è¡¨
        papers: è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ç¯‡è®ºæ–‡åº”åŒ…å«titleå’Œabstractå­—æ®µ
        inclusion_criteria: å¯é€‰çš„çº³å…¥æ ‡å‡†
        exclusion_criteria: å¯é€‰çš„æ’é™¤æ ‡å‡†
        output_dir: å¯é€‰çš„è¾“å‡ºç›®å½•
        top_n: å¯é€‰çš„ï¼Œè¾“å‡ºå‰Nç¯‡è®ºæ–‡ï¼ˆæŒ‰æœ€ç»ˆåˆ†æ•°æ’åºï¼‰
        threshold_mode: å¯é€‰çš„ï¼Œå…¥é€‰é—¨æ§›æ¨¡å¼ ("Sensitive", "Specific", "Balanced")
    
    Returns:
        åŒ…å«è¯„å®¡ç»“æœçš„å­—å…¸
    """
    return asyncio.run(run_lattereview_evaluation(
        topic=topic,
        reviewer_models=reviewer_models,
        papers=papers,
        inclusion_criteria=inclusion_criteria,
        exclusion_criteria=exclusion_criteria,
        output_dir=output_dir,
        top_n=top_n,
        threshold_mode=threshold_mode
    ))


# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # ç¤ºä¾‹æ•°æ®
    example_topic = "Large Language Models in Academic Research"
    example_reviewers = ["gpt-4o-mini", "gpt-4.1-mini", "gpt-5-mini"]
    example_papers = [
        {
            "id": "1",
            "title": "Example Paper 1",
            "abstract": "This is an example abstract for paper 1."
        },
        {
            "id": "2", 
            "title": "Example Paper 2",
            "abstract": "This is an example abstract for paper 2."
        }
    ]
    
    # è¿è¡Œè¯„å®¡
    result = run_lattereview_evaluation_sync(
        topic=example_topic,
        reviewer_models=example_reviewers,
        papers=example_papers,
        output_dir="./output"
    )
    
    print("è¯„å®¡ç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
